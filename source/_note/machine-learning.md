---
title: Note | Machine Learning
date: 2019-3-1
categories: Note
comments: false
tags:
- Design
---

《机器学习》。

<!-- more -->

原始数据通过特征工程变成特征向量。

分类问题：预测值有限

回归问题：预测值无限

聚类：非监督式学习

## 线性回归

训练集：`x`是输入特征向量，`y`是输出结果，`m`是训练集样例数量。

 凸函数：只有一个极小值点的函数。

梯度下降法：机器学习最基础的算法。

 一阶泰勒展开

* 初始化$\theta_1​$、$\theta_2​$
* 目标：$0 \le J(\theta_1) - J(\theta_1 + \Delta)​$
* 一阶泰勒展开$\approx J(\theta_1) - [J(\theta_1) + \frac{\partial J}{\partial\theta_1}\Delta] = -\frac{\partial J}{\partial\theta_1}\Delta \ge 0$

$\alpha > 0​$且$\approx​$要成立。

偏微分的正负号代表方向。

二阶泰勒展开 ，海森矩阵(二阶偏微分矩阵)。

模型的增量需要用上二阶泰勒展开。

极值点附近的点的偏导数接近于0。

批量梯度下降：每次更新依赖所有数据。

超平面，多维。

数量级差异，可能导致更新不同步。

正态分布：减去期望值，除以标准差。

奇异矩阵的求逆会报错，非奇异、满秩的矩阵才能求逆。非满秩矩阵可以求伪逆。

与梯度下降法相比，解方程法速度慢、准确率不高。

奇异矩阵的求逆给原矩阵加一个很小的系数的单位矩阵，然后再求逆。这种方法的解可能比伪逆更准确。

## 逻辑回归(对数几率回归)

分类

概率

验证函数是否是凸函数的方法，海森矩阵是半正定的。

$J(\theta)$是凸函数，梯度下降是没有风险的。

梯度下降基于一阶泰勒展开，L-BFGS是基于二阶泰勒展开。前者模型不稳定，后者模型稳定，业界使用多。

多类样本，比如三类，分别训练三种模型。

机器学习很大的问题：过拟合问题。

奥卡姆剃刀

训练集 -> 模型推广能力

$O(\sqrt{\frac{VC(dim)}{m}})$，m是训练样本数，VC(dim)是模型有效参数个数。

模型越复杂，推广能力越弱；训练集越小，推广能力越弱。

过拟合、正则化

SVM

两个向量的内积几何上的含义，投影，与夹角余弦值相关。







